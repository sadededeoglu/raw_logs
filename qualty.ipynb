{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe6a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "sys.path.append('/Users/sdedeoglu/Desktop/python/raw_logs')\n",
    "from log_pipeline import ParquetReader, ValidatorProcessor, UpsertWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501ec91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityChecker:\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.errors = []\n",
    "        \n",
    "    def check_null_values(self, table_name: str, critical_columns: List[str]) -> bool:\n",
    "        \"\"\"Kritik s√ºtunlarda null deƒüer kontrol√º\"\"\"\n",
    "        try:\n",
    "            null_conditions = \" OR \".join([f\"{col} IS NULL\" for col in critical_columns])\n",
    "            query = f\"SELECT COUNT(*) as null_count FROM {table_name} WHERE {null_conditions}\"\n",
    "            \n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(query)).fetchone()\n",
    "                null_count = result[0]\n",
    "                \n",
    "            if null_count > 0:\n",
    "                error_msg = f\" {table_name}: Found {null_count} null values in critical columns {critical_columns}\"\n",
    "                self.errors.append(error_msg)\n",
    "                print(error_msg)\n",
    "                return False\n",
    "            else:\n",
    "                print(f\" {table_name}: No null values in critical columns\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\" Error checking nulls in {table_name}: {str(e)}\"\n",
    "            self.errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return False\n",
    "    \n",
    "    def check_duplicates(self, table_name: str, unique_columns: List[str]) -> bool:\n",
    "        \"\"\"Duplicate record kontrol√º\"\"\"\n",
    "        try:\n",
    "            columns_str = \", \".join(unique_columns)\n",
    "            query = f\"\"\"\n",
    "            SELECT {columns_str}, COUNT(*) as cnt \n",
    "            FROM {table_name} \n",
    "            GROUP BY {columns_str}\n",
    "            HAVING COUNT(*) > 1\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "            \n",
    "            with self.engine.connect() as conn:\n",
    "                df = pd.read_sql(query, conn)\n",
    "                \n",
    "            if not df.empty:\n",
    "                error_msg = f\" {table_name}: Found {len(df)} duplicate records\"\n",
    "                self.errors.append(error_msg)\n",
    "                print(error_msg)\n",
    "                print(f\"Sample duplicates:\\n{df.head()}\")\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"{table_name}: No duplicate records found\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\" Error checking duplicates in {table_name}: {str(e)}\"\n",
    "            self.errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return False\n",
    "    \n",
    "    def check_data_freshness(self, table_name: str, date_column: str = \"updated_Date\") -> bool:\n",
    "        \"\"\"Bug√ºn√ºn verisi var mƒ± kontrol√º\"\"\"\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT COUNT(*) as today_count \n",
    "            FROM {table_name} \n",
    "            WHERE DATE({date_column}) = CURDATE()\n",
    "            \"\"\"\n",
    "            \n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(query)).fetchone()\n",
    "                today_count = result[0]\n",
    "                \n",
    "            if today_count == 0:\n",
    "                error_msg = f\"{table_name}: No fresh data found for today\"\n",
    "                self.errors.append(error_msg)\n",
    "                print(error_msg)\n",
    "                return False\n",
    "            else:\n",
    "                print(f\" {table_name}: Found {today_count} fresh records for today\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error checking freshness in {table_name}: {str(e)}\"\n",
    "            self.errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return False\n",
    "    \n",
    "    def check_schema_validation(self, df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
    "        \"\"\"DataFrame schema kontrol√º\"\"\"\n",
    "        try:\n",
    "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                error_msg = f\" Schema validation failed: Missing columns {missing_cols}\"\n",
    "                self.errors.append(error_msg)\n",
    "                print(error_msg)\n",
    "                return False\n",
    "            else:\n",
    "                print(f\" Schema validation passed: All required columns present\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\" Schema validation error: {str(e)}\"\n",
    "            self.errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return False\n",
    "    \n",
    "    def check_data_ranges(self, table_name: str, column_ranges: Dict[str, Dict]) -> bool:\n",
    "        \"\"\"Veri aralƒ±k kontrol√º\"\"\"\n",
    "        try:\n",
    "            all_passed = True\n",
    "            \n",
    "            for column, ranges in column_ranges.items():\n",
    "                min_val = ranges.get('min')\n",
    "                max_val = ranges.get('max')\n",
    "                \n",
    "                conditions = []\n",
    "                if min_val is not None:\n",
    "                    conditions.append(f\"{column} < {min_val}\")\n",
    "                if max_val is not None:\n",
    "                    conditions.append(f\"{column} > {max_val}\")\n",
    "                \n",
    "                if conditions:\n",
    "                    query = f\"SELECT COUNT(*) as invalid_count FROM {table_name} WHERE {' OR '.join(conditions)}\"\n",
    "                    \n",
    "                    with self.engine.connect() as conn:\n",
    "                        result = conn.execute(text(query)).fetchone()\n",
    "                        invalid_count = result[0]\n",
    "                    \n",
    "                    if invalid_count > 0:\n",
    "                        error_msg = f\" {table_name}.{column}: Found {invalid_count} values outside range [{min_val}, {max_val}]\"\n",
    "                        self.errors.append(error_msg)\n",
    "                        print(error_msg)\n",
    "                        all_passed = False\n",
    "                    else:\n",
    "                        print(f\" {table_name}.{column}: All values within valid range\")\n",
    "                        \n",
    "            return all_passed\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\" Error checking ranges in {table_name}: {str(e)}\"\n",
    "            self.errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "            return False\n",
    "    \n",
    "    def run_all_checks(self) -> bool:\n",
    "        \"\"\"T√ºm kontrolleri √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        print(\"üîç Starting Data Quality Checks...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Define checks for each table\n",
    "        checks = [\n",
    "            # Users table checks\n",
    "            self.check_null_values(\"users\", [\"user_id\"]),\n",
    "            self.check_duplicates(\"users\", [\"user_id\"]),\n",
    "            self.check_data_freshness(\"users\"),\n",
    "            \n",
    "            # Sessions table checks\n",
    "            self.check_null_values(\"sessions\", [\"session_id\", \"user_id\"]),\n",
    "            self.check_duplicates(\"sessions\", [\"session_id\"]),\n",
    "            self.check_data_freshness(\"sessions\"),\n",
    "            \n",
    "            # Events table checks\n",
    "            self.check_null_values(\"events\", [\"request_id\", \"session_id\", \"hotel_id\", \"funnel_id\"]),\n",
    "            self.check_duplicates(\"events\", [\"request_id\", \"session_id\", \"hotel_id\", \"funnel_id\"]),\n",
    "            self.check_data_freshness(\"events\"),\n",
    "            \n",
    "            # Hotels table checks\n",
    "            self.check_null_values(\"hotels\", [\"hotel_id\"]),\n",
    "            self.check_duplicates(\"hotels\", [\"hotel_id\"]),\n",
    "            self.check_data_ranges(\"hotels\", {\"hotel_price\": {\"min\": 0, \"max\": 50000}}),\n",
    "            \n",
    "            # Payments table checks\n",
    "            self.check_null_values(\"payments\", [\"request_id\"]),\n",
    "            self.check_duplicates(\"payments\", [\"request_id\"]),\n",
    "        ]\n",
    "        \n",
    "        # Run all checks\n",
    "        all_passed = all(checks)\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        if all_passed:\n",
    "            print(\"ALL DATA QUALITY CHECKS PASSED!\")\n",
    "        else:\n",
    "            print(f\" {len(self.errors)} DATA QUALITY ISSUES FOUND:\")\n",
    "            for error in self.errors:\n",
    "                print(f\"  ‚Ä¢ {error}\")\n",
    "                \n",
    "        return all_passed\n",
    "    \n",
    "    def get_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Data quality raporu d√∂nd√ºr\"\"\"\n",
    "        return {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_errors\": len(self.errors),\n",
    "            \"errors\": self.errors,\n",
    "            \"status\": \"PASSED\" if len(self.errors) == 0 else \"FAILED\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d8c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPipelineWithQuality:\n",
    "    def __init__(self, parquet_path: str, engine):\n",
    "        # Import etmek i√ßin gerekli sƒ±nƒ±flarƒ± import et\n",
    "        self.reader = ParquetReader(parquet_path)\n",
    "        self.validator = ValidatorProcessor()\n",
    "        self.writer = UpsertWriter(engine)\n",
    "        self.quality_checker = DataQualityChecker(engine)\n",
    "        self.now = datetime.now()\n",
    "\n",
    "    def clean_nan_values(self, df):\n",
    "        \"\"\"NaN deƒüerlerini MySQL uyumlu hale getir\"\"\"\n",
    "        # NaN deƒüerlerini None ile deƒüi≈ütir\n",
    "        df_cleaned = df.copy()\n",
    "        \n",
    "        # Numeric s√ºtunlarda NaN'larƒ± None ile deƒüi≈ütir\n",
    "        for col in df_cleaned.columns:\n",
    "            if df_cleaned[col].dtype in ['float64', 'float32']:\n",
    "                df_cleaned[col] = df_cleaned[col].replace({np.nan: None})\n",
    "        \n",
    "        # Object tipindeki s√ºtunlarda da NaN'larƒ± kontrol et\n",
    "        df_cleaned = df_cleaned.where(pd.notnull(df_cleaned), None)\n",
    "        \n",
    "        return df_cleaned\n",
    "\n",
    "    def run_once(self):\n",
    "        print(\"Starting Log Pipeline with Data Quality...\")\n",
    "        # 2. Data processing\n",
    "        df = self.reader.read_all()\n",
    "        df.dropna(subset=[\"user_id\", \"session_id\", \"hotel_id\", \"request_id\", \"funnel_id\"], inplace=True)\n",
    "        \n",
    "        # 3. Schema validation\n",
    "        required_columns = [\"user_id\", \"session_id\", \"hotel_id\", \"request_id\", \"funnel_id\"]\n",
    "        if not self.quality_checker.check_schema_validation(df, required_columns):\n",
    "            raise ValueError(\"Schema validation failed!\")\n",
    "        \n",
    "        # 4. Validate and transform\n",
    "        users, sessions, events, hotels, payments = self.validator.validate(df)\n",
    "\n",
    "        # 5. Add updated_Date and handle NaN values for MySQL compatibility\n",
    "        for tbl in (users, sessions, events, hotels, payments):\n",
    "            if not tbl.empty:\n",
    "                tbl[\"updated_Date\"] = self.now\n",
    "                # NaN deƒüerlerini MySQL uyumlu hale getir\n",
    "                tbl = self.clean_nan_values(tbl)\n",
    "\n",
    "        # 6. Write to database\n",
    "        print(\"\\n Writing to db\")\n",
    "        try:\n",
    "            self.writer.upsert_df(self.clean_nan_values(users), \"users\")\n",
    "            self.writer.upsert_df(self.clean_nan_values(sessions), \"sessions\")\n",
    "            self.writer.upsert_df(self.clean_nan_values(events), \"events\")\n",
    "            self.writer.upsert_df(self.clean_nan_values(hotels), \"hotels\")\n",
    "            self.writer.upsert_df(self.clean_nan_values(payments), \"payments\")\n",
    "        except Exception as e:\n",
    "            print(f\" Database write error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # 7. Post-processing data quality check\n",
    "        print(\"\\n Running post-processing data quality checks...\")\n",
    "        quality_passed = self.quality_checker.run_all_checks()\n",
    "        \n",
    "        if not quality_passed:\n",
    "            print(\"\\n  DATA QUALITY ISSUES DETECTED!\")\n",
    "            report = self.quality_checker.get_report()\n",
    "            print(f\"Quality Report: {report}\")\n",
    "        \n",
    "        print(\"\\n Pipeline completed\")\n",
    "        return quality_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f9cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Log Pipeline with Data Quality...\n",
      " Schema validation passed: All required columns present\n",
      "\n",
      " Writing to db\n",
      "\n",
      " Running post-processing data quality checks...\n",
      "üîç Starting Data Quality Checks...\n",
      "==================================================\n",
      " users: No null values in critical columns\n",
      "users: No duplicate records found\n",
      " users: Found 4001 fresh records for today\n",
      " sessions: No null values in critical columns\n",
      "sessions: No duplicate records found\n",
      " sessions: Found 99971 fresh records for today\n",
      " events: No null values in critical columns\n",
      "events: No duplicate records found\n",
      " events: Found 350590 fresh records for today\n",
      " hotels: No null values in critical columns\n",
      "hotels: No duplicate records found\n",
      " hotels.hotel_price: All values within valid range\n",
      " payments: No null values in critical columns\n",
      "payments: No duplicate records found\n",
      "==================================================\n",
      "ALL DATA QUALITY CHECKS PASSED!\n",
      "\n",
      " Pipeline completed\n",
      "\n",
      "============================================================\n",
      " STANDALONE DATA QUALITY CHECK\n",
      "============================================================\n",
      "üîç Starting Data Quality Checks...\n",
      "==================================================\n",
      " users: No null values in critical columns\n",
      "users: No duplicate records found\n",
      " users: Found 4001 fresh records for today\n",
      " sessions: No null values in critical columns\n",
      "sessions: No duplicate records found\n",
      " sessions: Found 99971 fresh records for today\n",
      " events: No null values in critical columns\n",
      "events: No duplicate records found\n",
      " events: Found 350590 fresh records for today\n",
      " hotels: No null values in critical columns\n",
      "hotels: No duplicate records found\n",
      " hotels.hotel_price: All values within valid range\n",
      " payments: No null values in critical columns\n",
      "payments: No duplicate records found\n",
      "==================================================\n",
      "ALL DATA QUALITY CHECKS PASSED!\n",
      "\n",
      "Pipeline Quality Result:  PASSED\n",
      "Standalone Quality Result:  PASSED\n"
     ]
    }
   ],
   "source": [
    "# Test pipeline\n",
    "try:\n",
    "    # Config y√ºkle\n",
    "    cfg_text = Path(\"/Users/sdedeoglu/Desktop/python/config.json\").read_text(encoding=\"utf-8\")\n",
    "    cfg = json.loads(cfg_text)\n",
    "    engine = create_engine(f\"mysql+pymysql://{cfg['kullanici']}:{cfg['sifre']}@{cfg['host']}:{cfg['port']}/{cfg['veritabani']}\")\n",
    "\n",
    "    # Pipeline √ßalƒ±≈ütƒ±r (data quality dahil)\n",
    "    pipeline = LogPipelineWithQuality(\"/Users/sdedeoglu/Desktop/python/case_data.parquet.gzip\", engine)\n",
    "    quality_passed = pipeline.run_once()\n",
    "\n",
    "    # Standalone data quality check\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" STANDALONE DATA QUALITY CHECK\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    quality_checker = DataQualityChecker(engine)\n",
    "    standalone_result = quality_checker.run_all_checks()\n",
    "\n",
    "    print(f\"\\nPipeline Quality Result: {' PASSED' if quality_passed else ' FAILED'}\")\n",
    "    print(f\"Standalone Quality Result: {' PASSED' if standalone_result else ' FAILED'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Pipeline Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
