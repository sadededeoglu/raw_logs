# Log Pipeline Project

Bu proje, parquet dosyalarÄ±ndan log verilerini okuyup MySQL veritabanÄ±na iÅŸleyen, veri kalitesi kontrolÃ¼ yapan ve test edilmiÅŸ bir ETL pipeline'Ä± iÃ§erir.

## ğŸ“ Proje YapÄ±sÄ±

```
raw_logs/
â”œâ”€â”€ log_pipeline.py         # Ana pipeline kodu
â”œâ”€â”€ unit_test.ipynb        # Birim testler
â”œâ”€â”€ qualty_test.ipynb      # Veri kalitesi testleri
â”œâ”€â”€ oop_result.ipynb       # Pipeline kullanÄ±m Ã¶rnekleri
â””â”€â”€ README.md              # Bu dosya
``` 

## ğŸš€ Ã–zellikler

- **Parquet Dosya Okuma**: BÃ¼yÃ¼k dosyalar iÃ§in batch iÅŸleme desteÄŸi
- **Veri DoÄŸrulama**: Pydantic ile tip gÃ¼venli validasyon
- **MySQL Upsert**: Duplicate key handling ile gÃ¼venli veri yazma
- **Veri Kalitesi KontrolÃ¼**: Null deÄŸer, duplicate, freshness kontrolleri
- **Birim Testler**: %100 test coverage
- **Chunk Ä°ÅŸleme**: BÃ¼yÃ¼k dosyalar iÃ§in bellek optimizasyonu

## ğŸ“‹ Gereksinimler

```python
pandas>=1.5.0
pydantic>=2.0.0
sqlalchemy>=2.0.0
pymysql>=1.0.0
numpy>=1.24.0
pathlib
```

## ğŸ—„ï¸ VeritabanÄ± TablolarÄ±

Pipeline 5 farklÄ± tablo oluÅŸturur:

- **users**: KullanÄ±cÄ± bilgileri
- **sessions**: Oturum verileri  
- **events**: Olay loglarÄ±
- **hotels**: Otel bilgileri
- **payments**: Ã–deme durumlarÄ±

## ğŸ”§ Kurulum

1. Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install pandas pydantic sqlalchemy pymysql numpy
```

2. MySQL veritabanÄ± yapÄ±landÄ±rmasÄ± iÃ§in `config.json` dosyasÄ± oluÅŸturun:
```json
{
    "kullanici": "your_username",
    "sifre": "your_password", 
    "host": "localhost",
    "port": 3306,
    "veritabani": "your_database"
}
```

## ğŸ“– KullanÄ±m

### Basit KullanÄ±m

```python
from pathlib import Path
import json
from sqlalchemy import create_engine
from log_pipeline import LogPipeline

# VeritabanÄ± baÄŸlantÄ±sÄ±
cfg = json.loads(Path("config.json").read_text())
engine = create_engine(f"mysql+pymysql://{cfg['kullanici']}:{cfg['sifre']}@{cfg['host']}:{cfg['port']}/{cfg['veritabani']}")

# Pipeline Ã§alÄ±ÅŸtÄ±r
pipeline = LogPipeline("data.parquet.gzip", engine)
pipeline.run_once()
```

### Batch Ä°ÅŸleme (BÃ¼yÃ¼k Dosyalar Ä°Ã§in)

```python
pipeline = LogPipeline("large_data.parquet.gzip", engine)

for batch in pipeline.reader.read_in_batches(batch_size=10000):
    batch.dropna(subset=["user_id", "session_id", "hotel_id", "request_id", "funnel_id"], inplace=True)
    users, sessions, events, hotels, payments = pipeline.validator.validate(batch)
    
    # Timestamp ekle
    now = pipeline.now
    for tbl in (users, sessions, events, hotels, payments):
        if not tbl.empty:
            tbl["updated_Date"] = now
    
    # VeritabanÄ±na yaz
    pipeline.writer.upsert_df(users, "users")
    pipeline.writer.upsert_df(sessions, "sessions") 
    pipeline.writer.upsert_df(events, "events")
    pipeline.writer.upsert_df(hotels, "hotels")
    pipeline.writer.upsert_df(payments, "payments")
```

### Veri Kalitesi KontrolÃ¼ Ä°le

```python
from qualty_test import LogPipelineWithQuality

pipeline = LogPipelineWithQuality("data.parquet.gzip", engine)
quality_passed = pipeline.run_once()

if not quality_passed:
    print("Veri kalitesi sorunlarÄ± tespit edildi!")
```

## ğŸ§ª Test Ã‡alÄ±ÅŸtÄ±rma

### Birim Testler

```python
# unit_test.ipynb dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n
import unittest
unittest.main(argv=[''], exit=False, verbosity=2)
```

Test edilen bileÅŸenler:
- âœ… ParquetReader (okuma ve batch iÅŸleme)
- âœ… ValidatorProcessor (veri doÄŸrulama)
- âœ… UpsertWriter (veritabanÄ± yazma)
- âœ… LogPipeline (end-to-end pipeline)

### Veri Kalitesi Testleri

```python
from qualty_test import DataQualityChecker

quality_checker = DataQualityChecker(engine)
quality_passed = quality_checker.run_all_checks()
```

Kontrol edilen kalite kriterleri:
- ğŸ” Null deÄŸer kontrolÃ¼
- ğŸ” Duplicate kayÄ±t kontrolÃ¼  
- ğŸ” Veri gÃ¼ncellik kontrolÃ¼
- ğŸ” Schema validasyonu
- ğŸ” Veri aralÄ±k kontrolÃ¼

## ğŸ“Š Veri Modelleri

### UserModel
```python
- user_id: float (zorunlu)
- subscriber_id: float (opsiyonel)
- country: str (opsiyonel)
- has_email_contact_permission: bool (opsiyonel)
- has_phone_contact_permission: bool (opsiyonel)
```

### EventModel
```python
- request_id: str (zorunlu)
- session_id: str (zorunlu)
- funnel_id: str (zorunlu)
- hotel_id: int (zorunlu)
- timestamp: datetime (zorunlu)
- page_name: str (opsiyonel)
- search_query: str (opsiyonel)
- destination_id: float (opsiyonel)
- num_guests: float (opsiyonel)
```

### HotelModel
```python
- hotel_id: int (zorunlu)
- hotel_price: float (opsiyonel)
- currency: str (opsiyonel)
```

### PaymentModel
```python
- request_id: str (zorunlu)
- payment_status: str (opsiyonel) # "pending", "success", "failed"
- confirmation_number: str (opsiyonel)
```

### SessionModel
```python
- session_id: str (zorunlu)
- user_id: float (opsiyonel)
- user_agent: str (opsiyonel)
- device_type: str (opsiyonel)
- ip_address: str (opsiyonel)
- utm_source: str (opsiyonel)
```

## âš™ï¸ YapÄ±landÄ±rma

### Boolean DeÄŸer DÃ¶nÃ¼ÅŸÃ¼mÃ¼
```python
true_set = {"yes", "true", "1", "y", "evet"}
false_set = {"no", "false", "0", "n", "hayir", "hayÄ±r"}
```

### Desteklenen Tarih FormatlarÄ±
```python
formats = [
    "%Y-%m-%d %H:%M:%S.%f",
    "%Y-%m-%d %H:%M:%S", 
    "%Y-%m-%dT%H:%M:%S.%fZ",
    "%Y-%m-%dT%H:%M:%S",
    "%d/%m/%Y %H:%M:%S",
    "%d-%m-%Y %H:%M:%S"
]
```

### Payment Status Mapping
```python
status = {
    "done": "success",
    "ok": "success", 
    "paid": "success",
    "fail": "failed",
    "error": "failed"
}
```

## ğŸ”’ Primary Key YapÄ±landÄ±rmasÄ±

```python
TABLE_PK_MAP = {
    "users": ["user_id"],
    "sessions": ["user_id", "session_id"],
    "events": ["request_id", "session_id", "hotel_id", "funnel_id"],
    "hotels": ["hotel_id"],
    "payments": ["request_id"]
}
```

## ğŸ› Hata YÃ¶netimi

- **Validation Errors**: GeÃ§ersiz veriler loglanÄ±r, pipeline devam eder
- **Database Errors**: Transaction rollback ile gÃ¼venli hata yÃ¶netimi
- **Schema Errors**: Missing column kontrolleri
- **Type Conversion Errors**: Graceful fallback mekanizmalarÄ±

## ğŸ“ˆ Performans

- **Chunk Size**: 1000 kayÄ±t (ayarlanabilir)
- **Batch Processing**: Bellek optimizasyonu iÃ§in bÃ¼yÃ¼k dosyalarda kullanÄ±n
- **Upsert Logic**: Efficient MySQL ON DUPLICATE KEY UPDATE
- **Connection Pooling**: SQLAlchemy engine ile otomatik yÃ¶netim